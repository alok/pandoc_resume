# Alok Singh

| <alokbeniwal@gmail.com> • 408-421-5658 •
  [github.com/alok](https://www.github.com/alok/)

## Publications

**Generalization Properties of Machine Learning-Based Weather Model
Downscaling**, ICLR 2020

**A Random Forest Model for the Probability of Large Wildfires in
California**, ICLR 2020

**Numerical Weather Model Super-Resolution**, NeurIPS 2019

**Spiky Corrupt Markov Decision Processes** IJCAI 2019

**Downscaling Numerical Weather Models with GANs** Climate Informatics
2019, AGU 2019, AMS 2019 (Oral)

## Education

2013 - 2017

:   **Bachelor's, Mathematics, UC Berkeley**

## Experience

**Deep Learning Scientist, Lawrence Berkeley National Lab** (2020)

ML lead for ARPA-E project on inverse design of metamaterials.

**Machine Learning Scientist, Terrafuse** (2019)

I work on deep learning for the physical sciences (e.g. fluid dynamics
and physical simulation) to create models that respect physical
constraints.

My work on using GANs for wind speed modeling was accepted to venues
such as NeurIPS and AGU. Other work on generative models to predict the
spread of a 3D fire simulation was presented at a climate science
symposium at UC Berkeley.

**Machine Learning Consultant, Papert Labs** (2018)

Consulted companies on how to use ML. Also worked with their engineering
teams on how to integrate predictive models into their overall
engineering system, replacing hand-tuned heuristics.

**Recurse Center** (2017)

Worked on deep reinforcement learning. Implemented DAgger, DQN, and
policy gradient methods such as parallel PPO. Also worked on
metaheuristics such as genetic algorithms and simulated annealing.

**Data Scientist Intern, Radius Intelligence** (2015)

I added customer data to internal company data, increasing total dataset
size by 3x. Apache Spark was used to do EDA.

## Projects

**Spiky CRMDPs**

Came up with algorithms for safe exploration in an environment with a
noisy reward function. Presented work at IJCAI 2019.

**Network Compression**

Implemented model compression to test the conclusion of the paper
*Understanding Deep Learning Requires Rethinking Generalization*. Blog
post and code
[here](https://alok.github.io/2018/01/12/compressing-neural-networks-to-see-if-they-learn).

## Skills

-   Python, PyTorch, TensorFlow, Bash, UNIX, Rust, Haskell
